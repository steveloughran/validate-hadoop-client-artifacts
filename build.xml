<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project name="download" default="dist" basedir=".">
  <description>
    build file to manage validation of artifacts.
    Maven is one of the targets here.

    hadoop version is set in release.properties;
    build.properties is required to set source of RC tarball

    All the complex commands are done by executing the unix commands;
    this build file sets them up by building the commands properly.

    for building other modules to work, this ant build must be on java11

    set -gx JAVA_HOME $JAVA11_HOME; and echo $JAVA_HOME
  </description>
  <!-- set global properties for this build -->
  <property name="src" location="src"/>
  <property name="home" location="${user.home}"/>
  <property name="target" location="target"/>
  <!--suppress AntResolveInspection -->
  <property file="build.properties"/>


  <property name="rc" value="RC1"/>

  <property name="dist.dir" location="${target}/dist"/>
  <property name="incoming.dir" location="${target}/incoming"/>


  <!--  base name of a release, 3.3.3-RC0 -->
  <property name="hadoop.version" value="3.3.3"/>
  <property name="rc.name" value="${hadoop.version}-${rc}"/>
  <property name="rc.name" value="${hadoop.version}-${rc}"/>

  <!-- for spark builds -->
  <property name="spark.version" value="3.4.0-SNAPSHOT"/>


  <property name="release" value="hadoop-${hadoop.version}"/>
  <property name="release.dir" location="${target}/${release}-${rc}"/>
  <property name="staged.artifacts.dir" location="${staging.dir}/${rc.name}"/>

  <property name="tag.name" value="release-${rc.name}"/>
  <property name="nexus.staging.url"
    value="https://repository.apache.org/content/repositories/orgapachehadoop-1349/"/>


  <target name="init">

    <presetdef name="x">
      <exec failonerror="true"/>
    </presetdef>

    <presetdef name="mvn">
      <x executable="mvn"/>
    </presetdef>

    <presetdef name="gpg">
      <x executable="gpg"/>
    </presetdef>

    <macrodef name="require">
      <attribute name="p" />
        <sequential>
          <fail unless="@{p}" message="unset property @{p}" />
        </sequential>
    </macrodef>


    <mkdir dir="${target}"/>

    <echo>
      hadoop.version=${hadoop.version}
      Fetching and validating artifacts in ${release.dir}
      staging to ${staging.dir}
      staged artifacts to ${staged.artifacts.dir}
      spark.dir = ${spark.dir}
      spark.version=${spark.version}
      cloud-examples.dir=${cloud-examples.dir}
      bigdata-interop.dir=${bigdata-interop.dir}
      hboss.dir=${hboss.dir}
    </echo>
  </target>

  <target name="clean"
    description="clean up target/ dir">
    <!-- Delete the ${dist} directory trees -->
    <delete dir="${target}"/>
  </target>

  <target name="purge" depends="init"
    description="purge all artifacts from the maven repo">
    <property name="mvn.repo"
      location="${user.home}/.m2/repository"/>
    <property name="hadoop.artifacts"
      location="${mvn.repo}/org/apache/hadoop"/>

    <echo>
      deleting ${hadoop.artifacts}/**/${hadoop.version}/*
    </echo>
    <delete>
      <fileset dir="${hadoop.artifacts}"
        includes="**/${hadoop.version}/*"/>
    </delete>

  </target>

  <target name="test" depends="init"
    description="build and test the maven module">

    <mvn>
      <arg value="test"/>
      <arg value="-Pstaging"/>
    </mvn>
  </target>


  <target name="scp-artifacts" depends="init"
    description="scp the artifacts from a remote host. may be slow">
    <fail unless="scp.hostname"/>
    <fail unless="scp.user"/>
    <fail unless="scp.hadoop.dir"/>
    <property name="scp.source"
      value="${scp.user}@${scp.hostname}:${scp.hadoop.dir}/target/artifacts"/>

    <delete dir="${incoming.dir}"/>
    <mkdir dir="${incoming.dir}"/>
    <!-- scp -r $srv:hadoop/target/artifacts ~/Projects/Releases
    -->
    <x executable="scp">
      <arg value="-r"/>
      <arg value="${scp.source}"/>
      <arg value="${incoming.dir}"/>
    </x>

  </target>


  <target name="move-scp-artifacts" depends="init"
    description="move the downloaded artifacts">
    <delete dir="${release.dir}"/>
    <move
      file="${incoming.dir}/artifacts"
      tofile="${release.dir}"/>
  </target>


  <target name="gpgv" depends="init"
    description="verify the downloaded artifacts">
    <fail message="dir missing: ${release.dir}">
      <condition>
        <not>
          <available file="${release.dir}"/>
        </not>
      </condition>
    </fail>

    <x executable="ls">
      <arg value="-l"/>
      <arg value="${release.dir}"/>
    </x>

    <presetdef name="gpgv">
      <gpg dir="${release.dir}">
      </gpg>
    </presetdef>

    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-src.tar.gz.asc"/>
    </gpgv>
    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-site.tar.gz.asc"/>
    </gpgv>
    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}.tar.gz.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-rat.txt.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="RELEASENOTES.md.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="CHANGELOG.md.asc"/>
    </gpgv>


  </target>

  <target name="stage" depends="init"
    description="copy the RC to the svn staging dir">

    <fail message="unset: ${staging.dir}"/>

    <move
      file="${release.dir}"
      todir="${staging.dir}"/>
    <x executable="ls">
      <arg value="-l"/>
      <arg value="${staging.dir}"/>
    </x>

  </target>

  <target name="print-tag-command"
    description="print the git command to tag the rc">
    <echo>
      command to tag the commit is

      git tag -s ${tag.name} -m "Release candidate -${rc.name}"
      git push apache ${tag.name}
    </echo>
  </target>


  <target name="vote-message"
    depends="init"
    description="build the vote message">


    <loadfile property="message.txt"
      srcFile="src/email.txt">
      <filterchain>
        <expandproperties/>
      </filterchain>
    </loadfile>
    <property name="message.out"
      location="${target}/email.txt"/>

    <echo>${message.txt}</echo>
    <echo file="${message.out}">${message.txt}</echo>
  </target>

  <target name="spark.build" if="spark.dir"
    depends="init"
    description="build the spark release in spark.dir">
    <echo>


      Note: this build includes kinesis and ASL artifacts
    </echo>
    <mvn dir="${spark.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Phadoop-cloud"/>
      <arg value="-Pyarn"/>
      <arg value="-Pkinesis-asl"/>
      <arg value="-DskipTests"/>
      <arg value="-Dmaven.javadoc.skip=true"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="install"/>
    </mvn>

  </target>


  <target name="cloud-examples.build" if="cloud-examples.dir"
    depends="init"
    description="build the cloud examples release">
    <echo>
      Build the cloud examples modules
    </echo>
    <mvn dir="${cloud-examples.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Dspark-3.4"/>
      <arg value="-Dspark.version=${spark.version}"/>
      <arg value="-DskipTests"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="install"/>
    </mvn>
  </target>


  <target name="cloud-examples.test"
    if="cloud-examples.dir"
    depends="init"
    description="test the cloud examples">
    <echo>
      Test the cloud examples;
      cloud.test.configuration.file must point to the xml file with binding info
    </echo>
    <require p="cloud.test.configuration.file" />
    <mvn dir="${cloud-examples.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Dspark-3.4"/>
      <arg value="-Dspark.version=${spark.version}"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="-Dcloud.test.configuration.file=${cloud.test.configuration.file}"/>
      <arg value="clean"/>
      <arg value="test"/>
    </mvn>
  </target>


  <target name="gcs.build" if="bigdata-interop.dir"
    depends="init"
    description="Build the google gcs artifacts">
    <echo>
      Build the google gcs artifacts.

      requires bigdata-interop.dir to be set to the base
      of a copy of
      https://github.com/GoogleCloudPlatform/bigdata-intero
    </echo>
    <mvn dir="${bigdata-interop.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-DskipTests"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="package"/>
      <arg value="install"/>
    </mvn>
  </target>

  <target name="hboss.build" if="hboss.dir"
    depends="init"
    description="Build the google gcs artifacts">
    <echo>
      Build the HBase HBoss module.
      It's test are brittle to s3a internal changes, just because
      it needs to plug in its own s3 client.

      asf-staging is a profile in stevel's ~/.m2/settings.xml to
      use the asf staging reop.
    </echo>
    <mvn dir="${hboss.dir}">
      <arg value="-Pasf-staging"/>
      <arg value="-DskipTests"/>
      <arg value="-Dhadoop33.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="install"/>
    </mvn>
  </target>

</project>
