<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project name="download" default="dist" basedir=".">
  <description>
    build file to manage validation of artifacts.
    Maven is one of the targets here.

    hadoop version is set in the property hadoop.version
    build.properties is required to set source of RC tarball

    All the complex commands are done by executing the unix commands;
    this build file sets them up by building the commands properly.

    for building other modules to work, this ant build must be on java11

    set -gx JAVA_HOME $JAVA11_HOME; and echo $JAVA_HOME

    or in fish

    For validating artifacts put up as an an RC, use the http-artifacts target
    to retrieve, with http.source set to the url, e.g
    http.source=https://home.apache.org/~iwasakims/hadoop-2.10.2-RC0/
  </description>
  <!-- set global properties for this build -->
  <property name="src" location="src"/>
  <property name="home" location="${user.home}"/>
  <property name="target" location="target"/>
  <!--suppress AntResolveInspection -->
  <property file="build.properties"/>


  <property name="rc" value="RC1"/>

  <property name="dist.dir" location="${target}/dist"/>
  <property name="incoming.dir" location="${target}/incoming"/>


  <!--  base name of a release, 3.3.3-RC0 -->
  <property name="hadoop.version" value="3.3.3"/>
  <property name="rc.name" value="${hadoop.version}-${rc}"/>

  <!-- for spark builds -->
  <property name="spark.version" value="3.4.0-SNAPSHOT"/>


  <property name="release" value="hadoop-${hadoop.version}"/>
  <property name="release.dir" location="${target}/${release}-${rc}"/>
  <property name="staged.artifacts.dir" location="${staging.dir}/${rc.name}"/>

  <property name="tag.name" value="release-${rc.name}"/>
  <property name="nexus.staging.url"
    value=""/>
  <property name="release.untar.dir" location="${target}/untar"/>
  <property name="release.source.dir" location="${release.untar.dir}/source"/>
  <property name="release.bin.dir" location="${release.untar.dir}/bin"/>
  <property name="release.native.binaries" value="true"/>


  <target name="init">

    <presetdef name="x">
      <exec failonerror="true"/>
    </presetdef>

    <presetdef name="mvn">
      <x executable="mvn"/>
    </presetdef>

    <presetdef name="gpg">
      <x executable="gpg"/>
    </presetdef>


    <macrodef name="require-dir">
      <attribute name="dir" />
        <sequential>
          <fail message="dir missing: @{dir}">
            <condition>
              <not>
                <available file="@{dir}"/>
              </not>
            </condition>
          </fail>

        </sequential>
    </macrodef>

    <presetdef name="verify-release-dir">
      <require-dir dir="${release.dir}" />
    </presetdef>

    <macrodef name="require">
      <attribute name="p" />
        <sequential>
          <fail unless="@{p}" message="unset property @{p}" />
        </sequential>
    </macrodef>


    <mkdir dir="${target}"/>

    <property name="scp.source"
      value="${scp.user}@${scp.hostname}:${scp.hadoop.dir}/target/artifacts"/>

    <echo>
      hadoop.version=${hadoop.version}
      rc=${rc}

      Fetching and validating artifacts in ${release.dir}
      release.dir=${release.dir}

      scp.source=${scp.source}
      http.source=${http.source}

      release.source.dir=${release.source.dir}
      staging.dir=${staging.dir}
      staged.artifacts.dir=${staged.artifacts.dir}

      spark.dir = ${spark.dir}
      spark.version=${spark.version}

      cloudstore.dir=${cloudstore.dir}
      bigdata-interop.dir=${bigdata-interop.dir}
      hboss.dir=${hboss.dir}
      cloud-examples.dir=${cloud-examples.dir}
      cloud.test.configuration.file=${cloud.test.configuration.file}

    </echo>
  </target>

  <target name="clean"
    description="clean up target/ dir">
    <!-- Delete the ${dist} directory trees -->
    <delete dir="${target}"/>
  </target>

  <target name="purge" depends="init"
    description="purge all artifacts from the maven repo">
    <property name="mvn.repo"
      location="${user.home}/.m2/repository"/>
    <property name="hadoop.artifacts"
      location="${mvn.repo}/org/apache/hadoop"/>

    <echo>
      deleting ${hadoop.artifacts}/**/${hadoop.version}/*
    </echo>
    <delete>
      <fileset dir="${hadoop.artifacts}"
        includes="**/${hadoop.version}/*"/>
    </delete>

  </target>

  <target name="mvn-test" depends="init"
    description="build and test the maven module">

    <mvn>
      <arg value="test"/>
      <arg value="-Pstaging"/>
    </mvn>
  </target>


  <target name="scp-artifacts" depends="init"
    description="scp the artifacts from a remote host. may be slow">
    <fail unless="scp.hostname"/>
    <fail unless="scp.user"/>
    <fail unless="scp.hadoop.dir"/>
    <property name="scp.source"
      value="${scp.user}@${scp.hostname}:${scp.hadoop.dir}/target/artifacts"/>

    <delete dir="${incoming.dir}"/>
    <mkdir dir="${incoming.dir}"/>
    <!-- scp -r $srv:hadoop/target/artifacts ~/Projects/Releases
    -->
    <x executable="scp">
      <arg value="-r"/>
      <arg value="${scp.source}"/>
      <arg value="${incoming.dir}"/>
    </x>

  </target>


  <target name="move-scp-artifacts" depends="init"
    description="move the downloaded artifacts">
    <delete dir="${release.dir}"/>
    <move
      file="${incoming.dir}/artifacts"
      tofile="${release.dir}"/>
  </target>

  <target name="release.dir.check" depends="init">
    <verify-release-dir />


    <x executable="ls">
      <arg value="-l"/>
      <arg value="${release.dir}"/>
    </x>

  </target>


  <target name="gpg.keys" depends="init"
    description="fetch GPG keys">

    <gpg>
      <arg value="--fetch-keys"/>
      <arg value="https://downloads.apache.org/hadoop/common/KEYS"/>
    </gpg>
  </target>

  <target name="gpg.verify" depends="release.dir.check"
    description="verify the downloaded artifacts">
    <presetdef name="gpgv">
      <gpg dir="${release.dir}">
      </gpg>
    </presetdef>

    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-src.tar.gz.asc"/>
    </gpgv>
    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-site.tar.gz.asc"/>
    </gpgv>
    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}.tar.gz.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="${release}-rat.txt.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="RELEASENOTES.md.asc"/>
    </gpgv>

    <gpgv>
      <arg value="--verify"/>
      <arg value="CHANGELOG.md.asc"/>
    </gpgv>


  </target>

  <target name="stage" depends="init"
    description="copy the RC to the svn staging dir">

    <fail message="unset: ${staging.dir}"/>

    <move
      file="${release.dir}"
      todir="${staging.dir}"/>
    <x executable="ls">
      <arg value="-l"/>
      <arg value="${staging.dir}"/>
    </x>

  </target>

  <target name="print-tag-command"
    description="print the git command to tag the rc">
    <echo>
      command to tag the commit is

      git tag -s ${tag.name} -m "Release candidate -${rc.name}"
      git push apache ${tag.name}
    </echo>
  </target>


  <target name="vote-message"
    depends="init"
    description="build the vote message">


    <loadfile property="message.txt"
      srcFile="src/email.txt">
      <filterchain>
        <expandproperties/>
      </filterchain>
    </loadfile>
    <property name="message.out"
      location="${target}/email.txt"/>

    <echo>${message.txt}</echo>
    <echo file="${message.out}">${message.txt}</echo>
  </target>

  <target name="spark.build" if="spark.dir"
    depends="init"
    description="build the spark release in spark.dir">
    <echo>

      Note: this build includes kinesis and ASL artifacts
    </echo>
    <mvn dir="${spark.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Phadoop-cloud"/>
      <arg value="-Pyarn"/>
      <arg value="-Pkinesis-asl"/>
      <arg value="-DskipTests"/>
      <arg value="-Dmaven.javadoc.skip=true"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="install"/>
    </mvn>

  </target>


  <target name="cloud-examples.build" if="cloud-examples.dir"
    depends="init"
    description="build the cloud examples release">
    <echo>
      Build the cloud examples modules
    </echo>

  </target>


  <target name="cloud-examples.test"
    if="cloud-examples.dir"
    depends="init"
    description="test the cloud examples">
    <echo>
      Test the cloud examples;
      cloud.test.configuration.file must point to the xml file with binding info
    </echo>
    <require p="cloud.test.configuration.file" />
    <mvn dir="${cloud-examples.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Dspark-3.4"/>
      <arg value="-Dspark.version=${spark.version}"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="-Dcloud.test.configuration.file=${cloud.test.configuration.file}"/>
      <arg value="clean"/>
      <arg value="test"/>
    </mvn>
  </target>


  <target name="gcs.build" if="bigdata-interop.dir"
    depends="init"
    description="Build the google gcs artifacts">
    <echo>
      Build the google gcs artifacts.

      requires bigdata-interop.dir to be set to the base
      of a copy of
      https://github.com/GoogleCloudPlatform/bigdata-intero
    </echo>
    <mvn dir="${bigdata-interop.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-DskipTests"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="package"/>
      <arg value="install"/>
    </mvn>
  </target>

  <target name="hboss.build" if="hboss.dir"
    depends="init"
    description="Build the hboss artifacts">
    <echo>
      Build the HBase HBoss module.
      It's test are brittle to s3a internal changes, just because
      it needs to plug in its own s3 client.

      asf-staging is a profile in stevel's ~/.m2/settings.xml to
      use the asf staging reop.
    </echo>
    <mvn dir="${hboss.dir}">
      <arg value="-Pasf-staging"/>
      <arg value="-DskipTests"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="-Dhadoop33.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="install"/>
    </mvn>
  </target>


  <target name="cloudstore.build" if="cloudstore.dir"
    depends="init"
    description="Build the cloudstore artifacts">
    <echo>
      Build the cloudstore module.
      if this is done with java11, it shouldn't be released.

    </echo>
    <mvn dir="${cloudstore.dir}">
      <arg value="-Psnapshots-and-staging"/>
      <arg value="-Pextra"/>
      <arg value="-Dhadoop.version=${hadoop.version}"/>
      <arg value="clean"/>
      <arg value="package"/>
    </mvn>
  </target>


  <!--  Fetch the artifacts from an http repo, for validating someone else's release.
   the download is into incoming.dir, then after a cleanup copied into release.dir; -->
  <target name="release.fetch.http" depends="init"
    description="fetch the artifacts from a remote http site with wget. may be slow">
    <fail unless="http.source"/>


    <delete dir="${incoming.dir}"/>
    <mkdir dir="${incoming.dir}"/>
    <!-- list and then wget the immediate children  into the incoming dir -->
    <x executable="wget" dir="${incoming.dir}" >
      <arg value="--no-parent"/>
      <arg value="--recursive"/>
      <arg value="--level=1"/>
      <arg value="--no-directories"/>
      <arg value="${http.source}"/>
    </x>
    <!--  remove all index.html files which crept in -->
    <delete dir="${incoming.dir}" includes="index.*" />

    <delete dir="${release.dir}"/>
    <move
      file="${incoming.dir}"
      tofile="${release.dir}"/>
  </target>

  <target name="release.src.untar" depends="release.dir.check"
    description="untar the release">

    <gunzip src="${release.dir}/${release}-src.tar.gz" dest="target/untar"/>
    <untar src="target/untar/${release}-src.tar" dest="${release.source.dir}" />
  </target>

  <target name="release.src.build" depends="init"
    description="build the release; call release.src.untar if needed">
    <mvn dir="${release.source.dir}/${release}-src">
      <arg value="clean"/>
      <arg value="install"/>
      <arg value="-DskipTests"/>
    </mvn>
  </target>

  <target name="release.src.test" depends="init"
    description="test the release; call release.src.untar if needed">
    <mvn dir="${release.source.dir}/${release}-src">
      <arg value="clean"/>
      <arg value="test"/>
    </mvn>
  </target>

  <target name="release.bin.untar" depends="release.dir.check"
    description="untar the release">

    <gunzip src="${release.dir}/${release}.tar.gz" dest="target/untar"/>

    <!--  use the native command to preserve properties -->
    <x executable="tar" dir="target/untar" >
      <arg value="-xf" />
      <arg value="${release}.tar" />
    </x>
    <echo>
      Binary release expanded into target/untar/${release}
    </echo>
  </target>

  <target name="release.bin.commands" depends="init"
    description="run test hadoop commands ">


    <!--   hadoop with errors-->
    <presetdef name="hadoop">
      <exec failonerror="true"
        executable="bin/hadoop"
        dir="target/untar/${release}" />
    </presetdef>

    <!--    quiet hadoop-->
    <presetdef name="hadoopq">
      <exec failonerror="false"
        executable="bin/hadoop"
        dir="target/untar/${release}" />
    </presetdef>
    <echo>ls</echo>
    <hadoop>
      <arg value="fs" />
      <arg value="-ls" />
      <arg value="file://${target}" />
    </hadoop>

    <echo>du</echo>
    <hadoop>
      <arg value="fs" />
      <arg value="-du" />
      <arg value="-h" />
      <arg value="file://${target}" />
    </hadoop>

    <echo>checknative</echo>

    <hadoopq failonerror="${release.native.binaries}">
      <arg value="checknative" />
    </hadoopq>

  </target>
</project>
